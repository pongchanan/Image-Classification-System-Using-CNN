{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CMD section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\pongc\\appdata\\roaming\\python\\python312\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\pongc\\appdata\\roaming\\python\\python312\\site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\pongc\\appdata\\roaming\\python\\python312\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\pongc\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\pongc\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\pongc\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pongc\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pongc\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pongc\\appdata\\roaming\\python\\python312\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\pongc\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\pongc\\appdata\\roaming\\python\\python312\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\pongc\\appdata\\roaming\\python\\python312\\site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\pongc\\appdata\\roaming\\python\\python312\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pongc\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# for build nural network\n",
    "import torch.nn as nn\n",
    "\n",
    "# for adjust the weights of model\n",
    "import torch.optim as optim\n",
    "\n",
    "# for convert convert pytorch data to useable data \n",
    "# for perform mathematical operations\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocess and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare transform for image\n",
    "data_transform = transforms.Compose([\n",
    "    # Ensure all images are the same size as required by the model\n",
    "    transforms.Resize((256, 256)), \n",
    "    \n",
    "    # convert the image to a multidimensional array which good for gpu\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # convert image value from 0-255 to 0-1 for improve stability of model\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply transform to dataset\n",
    "datasets = datasets.ImageFolder(root='../data/train', transform=data_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define size of each dataset\n",
    "train_size = int(0.7 * len(datasets))\n",
    "val_size = int(0.2 * len(datasets))\n",
    "test_size = len(datasets) - train_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into batches\n",
    "train_dataset, val_dataset, test_dataset = random_split(datasets, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders\n",
    "# each time it will load 32 images for balance between speed and memory usage\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of a batch\n",
    "def show_batch(loader):\n",
    "    batch = next(iter(loader))\n",
    "    images, labels = batch\n",
    "    fig, ax = plt.subplots(ncols=4, figsize=(20, 20))\n",
    "    for idx, img in enumerate(images[:4]):\n",
    "        ax[idx].imshow(img.permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
    "        ax[idx].title.set_text(f'Label: {labels[idx].item()}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture\n",
    "class EmotionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmotionCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Kernel_size is a size od window that will slide over the image\n",
    "            # Padding is the number of pixels that will be added to the image for the kernel to slide over\n",
    "            # MaxPool2d is a function that will reduce the size of the image by taking the n maximum value of a window\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            # Flatten is a function that will convert a multidimensional array to a one-dimensional array to be used by the linear layer\n",
    "            # linear layer is a layer to make large number of input to small number of output\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*32*32, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # pass the image through the convolutional layers\n",
    "        x = self.conv_layers(x)\n",
    "        # pass the output of the convolutional layers to the classifier\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training setup\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = EmotionCNN().to(device)\n",
    "# Loss function is a function that will calculate the difference between the predicted value and the actual value\n",
    "criterion = nn.BCELoss()\n",
    "# Optimizer is a function that will adjust the weights of the model to minimize the loss\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.float().to(device).unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.float().to(device).unsqueeze(1)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs} - Train Loss: {train_loss/len(train_loader)} - Val Loss: {val_loss/len(val_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    true_negative = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.squeeze() == labels).sum().item()\n",
    "            true_positive += ((predicted == 1) & (labels == 1)).sum().item()\n",
    "            false_positive += ((predicted == 1) & (labels == 0)).sum().item()\n",
    "            false_negative += ((predicted == 0) & (labels == 1)).sum().item()\n",
    "            true_negative += ((predicted == 0) & (labels == 0)).sum().item()\n",
    "    \n",
    "    precision = true_positive / (true_positive + false_positive) if true_positive + false_positive > 0 else 0\n",
    "    recall = true_positive / (true_positive + false_negative) if true_positive + false_negative > 0 else 0\n",
    "    accuracy = correct / total\n",
    "\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}% - Precision: {precision:.2f} - Recall: {recall:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction a single image\n",
    "def predict_image(model, image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_resized = cv2.resize(img_rgb, (256, 256))\n",
    "    \n",
    "    img_tensor = transforms.ToTensor()(img_resized)\n",
    "    img_normalized = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(img_tensor)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img_batch = img_normalized.unsqueeze(0).to(device)\n",
    "        prediction = model(img_batch)\n",
    "    \n",
    "    if prediction.item() < 0.5:\n",
    "        print('Happy')\n",
    "    else:\n",
    "        print('Sad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Train Loss: 0.004167568795310217 - Val Loss: 1.389678943157196\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../saved_models/emotion_cnn.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "save_model(model, '../saved_models/emotion_cnn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.71% - Precision: 0.47 - Recall: 0.56\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../saved_models/emotion_cnn.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pongc\\AppData\\Local\\Temp\\ipykernel_48804\\3702172722.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EmotionCNN(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=16384, out_features=256, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (4): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model (for future inference or training)\n",
    "model = EmotionCNN().to(device)\n",
    "load_model(model, '../saved_models/emotion_cnn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy\n"
     ]
    }
   ],
   "source": [
    "# Predict on a test image\n",
    "predict_image(model, '../data/test/Laugh_face.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
